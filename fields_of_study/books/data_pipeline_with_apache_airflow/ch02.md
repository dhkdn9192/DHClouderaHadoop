# Chapter 02. Airflow DAG의 구조

### Airflow DAG 작성 예제
<img width="672" alt="image" src="https://github.com/dhkdn9192/data_engineer_career/assets/11307388/005f9f45-c369-4b0a-a396-5717b8b53aaf">


```python
dag = DAG(
    dag_id="download_rocket_launches",
    description="Download rocket pictures of recently launched rockets.",
    start_date=airflow.utils.dates.days_ago(14),
    schedule_interval="@daily",
)

download_launches = BashOperator(
    task_id="download_launches",
    bash_command="curl -o /tmp/launches.json -L 'https://ll.thespacedevs.com/2.0.0/launch/upcoming'",  # noqa: E501
    dag=dag,
)

def _get_pictures():
    # Ensure directory exists
    pathlib.Path("/tmp/images").mkdir(parents=True, exist_ok=True)

    # Download all pictures in launches.json
    with open("/tmp/launches.json") as f:
        launches = json.load(f)
        image_urls = [launch["image"] for launch in launches["results"]]
        for image_url in image_urls:
            try:
                response = requests.get(image_url)
                image_filename = image_url.split("/")[-1]
                target_file = f"/tmp/images/{image_filename}"
                with open(target_file, "wb") as f:
                    f.write(response.content)
                print(f"Downloaded {image_url} to {target_file}")
            except requests_exceptions.MissingSchema:
                print(f"{image_url} appears to be an invalid URL.")
            except requests_exceptions.ConnectionError:
                print(f"Could not connect to {image_url}.")

get_pictures = PythonOperator(
    task_id="get_pictures", python_callable=_get_pictures, dag=dag
)

notify = BashOperator(
    task_id="notify",
    bash_command='echo "There are now $(ls /tmp/images/ | wc -l) images."',
    dag=dag,
)

download_launches >> get_pictures >> notify
```

### 태스크 실행 순서 정의
```
download_launches >> get_pictures >> notify
```
- Airflow에선 **오른쪽 시프트 연산자** (binary right shift operator), 즉 "rshift"(>>)를 사용하여 태스크 간 의존성을 정의한다.
- 파이썬에서 rshift 연산자는 비트를 이동하는데 사용된다.
- 그러나 Airflow에선 비트연산자를 사용할 일이 없으므로 위와 같은 용도로 재정의하여 사용하고 있다.

### 태스크와 오퍼레이터의 차이점
- 오퍼레이터는 **단일 작업 수행 역할**을 한다.
- 교재 및 Airflow 공식문서에선 오퍼레이터와 태스크라는 용어를 **같은 의미로 사용**한다. (사용자 관점에선 모두 같은 의미)
- 하지만 두 용어에는 차이점이 존재한다.
  - **오퍼레이터**: Airflow 사용자가 이용하는 다양한 서브 클래스 (PythonOperator, EmailOperator 등)
  - **태스크**: 오퍼레이터의 상태를 관리하는 Airflow의 내장 컴포넌트
- DAG는 이러한 오퍼레이터 집합의 수행을 오케스트레이션한다.
<img width="570" alt="image" src="https://github.com/dhkdn9192/data_engineer_career/assets/11307388/3c5187b7-4f2c-46ab-8f40-d33397f6fd2a">

### Airflow에서 DAG 실행하기
- 파이썬 환경에서 Airflow 실행
  - Airflow가 2016년 아파치 재단에 가입하면서 airflow가 아닌 apache-airflow로 설치하도록 변경되었다. `pip install apache-airflow`
  - 특정 파이썬 환경을 유지하고 재사용할 수 있도록 pyenv, conda, virtualenv 등 도구로 환경을 생성하는 것이 좋다.
  - 절차: Airflow 설치 > 메타스토어 초기화 > 사용자 생성 > DAG를 DAG 디렉토리에 복사 > 스케줄러와 웹 서버 실행
```bash
airflow db init
airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org
cp download_rocket_launches.py ~/airflow/dags/
airflow webserver
airflow scheduler
```  

- 도커 컨테이너에서 Airflow 실행
  - pyenv 등 도구는 파이썬 실행 수준에서 실행 환경을 격리하지만, 도커 컨테이너는 **OS 수준에서 실행 환경을 격리**한다.
  - 파이썬 패키지, DB Driver, GCC Compiler 등 의존성을 포함한 도커 컨테이너를 생성할 수 있다.
```bash
docker run \
  -ti \
  -p 8080:8080 \
  -v /path/to/dag/download_rocket_launches.py:/opt/airflow/dags/download_rocket_launches.py \
  --entrypoint=/bin/bash \
  --name airflow \
  apache/airflow:2.0.0-python3.8 \
  -c '( \
    airflow db init && \
    airflow users create \
      --username admin \
      --password admin \
      --firstname Anonymous \
      --lastname Admin \
      --role Admin \
      --email admin@example.org \
  ); \
airflow webserver & \
airflow scheduler \
'
```
