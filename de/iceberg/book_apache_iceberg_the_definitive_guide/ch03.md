# Chapter 03. Lifecycle of Write and Read Queries

(Chapter 2 복기) Iceberg는 다음과 같이 3개의 계층으로 구분된다.
![image](https://github.com/user-attachments/assets/1c846902-39c5-4355-8ec8-2f03a9edaadb)

**카탈로그 계층**
- 카탈로그는 current metadata pointer를 참조하며 이는 각 테이블의 최신 메타데이터 파일을 의미한다.
- 쿼리엔진은 읽기든 쓰기든 상관없이 가장 먼저 카탈로그와 상호작용한다.
  - 읽기의 경우: 테이블의 현재 상태를 확인하기 위해 카탈로그를 참조
  - 쓰기의 경우: 정의된 스키마를 준수하고 파티셔닝 스키마를 확인하기 위해 카탈로그를 참조

**메타데이터 계층**
- 메타데이터 파일, manifest 리스트, manifest 파일로 구성된다.
- 쿼리엔진이 테이블에 쓰기를 수행할 때 마다 새로운 메타데이터 파일이 자동 생성되고 가장 최신버전으로 정의된다.
  - 이는 테이블 커밋의 선형기록을 보장하고 동시쓰기와 같은 상황을 해결할 수 있게 해준다.
  - 또한 읽기 작업 시 항상 테이블의 최신 버전을 볼 수 있게 해준다.
- 쿼리엔진은 manifest 리스트를 통해 읽을 필요없는  manifest 파일들을 skip하여 빠른 성능을 낼 수 있다.
- 쿼리엔진은 manifest 파일의 정보를 통해 파일 프루닝을 할 수 있다.
  - 칼럼별 상한 및 하한, null 값 카운트, 파티션 데이터 등

**데이터 계층**
- 쿼리엔진은 파티셔닝 쿼리에 필요한 데이터파일을 읽기 위해 메타데이터 파일을 필터링한다.
- 쓰기 작업에선 데이터파일이 파일 스토리지에 작성되고, 이어서 관련된 메타데이터 파일이 생성 및 업데이트된다.




## Writing Queries in Apache Iceberg

Iceberg의 쓰기는 다음과 같이 수행된다.
- 쓰기 쿼리가 시작되면 쿼리엔진이 이를 파싱한다.
- 쿼리엔진은 데이터의 일관성과 무결성을 보장하고 데이터를 파티션 전략에 맞게 쓰기 위해 카탈로그를 참고한다.
- 데이터파일과 메타데이터파일이 쿼리에 따라 작성된다.
- 최종적으로 카탈로그 파일이 최신 메타데이터를 반영하도록 업데이트된다. 이후로 수행되는 읽기 작업은 가장 최신 버전의 데이터에 접근하게 된다.

![image](https://github.com/user-attachments/assets/fc543a7c-a174-4dd5-bf4f-a9cb71d42cd7)


### Create the Table


```sql
# Spark SQL
CREATE TABLE orders (
order_id BIGINT,
customer_id BIGINT, order_amount DECIMAL(10, 2), order_ts TIMESTAMP
)
USING iceberg
PARTITIONED BY (HOUR(order_ts))
```

hdfs의 테이블 경로 하위에 metadata 디렉토리가 있고 그 안에 메타데이터 json 파일이 존재

```json
{
  "table-uuid" : "....",
  "current-schema-id" : 0,
  "schemas" : [ {
    "type" : "struct",
    "schema-id" : 0,
    "fields" : [ {
      "id" : 1,
      "name" : "order_id",
      "required" : false,
      "type" : "long"
    }, {
      "id" : 2,
      "name" : "customer_id",
      "required" : false,
      "type" : "long"
    }, {
      "id" : 3,
      "name" : "order_amount",
      "required" : false,
      "type" : "decimal(10, 2)"
    }, {
      "id" : 4,
      "name" : "order_ts",
      "required" : false,
      "type" : "timestamptz"
    } ]
  } ],
  "partition-specs" : [ {
    "spec-id" : 0,
    "fields" : [ {
      "name" : "order_ts_hour",
      "transform" : "hour",
      "source-id" : 4,
      "field-id" : 1000
    } ]
  } ],
  "last-partition-id" : 1000,
  "current-snapshot-id" : -1,
  ...
}
```



### Insert the Query

### Merge Query




## Reading Queries in Apache Iceberg

### The SELECT Query

### The Time-Travel Query



## Conclusion
